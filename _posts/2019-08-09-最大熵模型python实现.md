---
title: "最大熵模型Python实现"
date: 2019-08-09
layout: post
mathjax: true
categories: 
tags: 
---



# 引言

   最大熵(maximum entropy model, MaxEnt)是一种典型的分类算法，在NLP、图像处理等领域有着广泛的应用。
有关最大熵的概念可以追溯到很早以前，它反映了人类认识世界的一个朴素原则，即对某个事件一无所知的情况下，选择一个
模型使它的分布应该尽可能均匀。而现实世界中我们面对的更多的问题是，在已知事件的许多先验知识的情况下，如何选择一个
合适的模型来对事件做出预测。最大熵模型正是用来解决这个问题的。




# 最大熵模型

最大熵模型是用来进行概率估计的。假设 $a$ 是某个事件，$b$ 是事件 $a$ 发生的环境（上下文），我们想知道 $a$ 和 $b$ 的联合概率，记
为 $p(a,b)$ .更一般地，设所有可能发生的事件组成的集合为 $A$ ，所有环境组成的集合为 $B$ ，我们想知道，对于任意给定的
$a\in A$ , $b \in B$ , 概率 $p(a,b)$ 是多少，即联合分布 $p(a,b)$ 的经验分布。

根据Shannon的定义，熵的计算公式为：

$$H(p) = - \sum_{x}(x)log_2 p(x)$$

那么，求解满足最大熵原则的概率分布的公式为：

$$p^{*} = \text{arg}max_{p \in P} \; H(p)$$

如果没有其他任何先验知识，根据熵的性质，上式的最大值的条件是：

$$p(a|b) = \frac{1}{|A|}$$
因为，
$$\sum_{p \in P} p(a|b) = 1$$ .

这里表示的是对于给定的 $b$ ，以条件概率 $$p(a|b)$$ 
得出 
$$a$$ 的概率。对于给定的一个训练集

$$T = \{(a_1,b_1),(a_2,b_2),\cdots,(a_n, b_n)\}$$

我们的目的就是用最大熵原理来作出最好的分类模型。



引入特征函数的概念，特征函数一般是一个二值函数 $f(a,b) -> \\{0,1\\}$。用来描述给定 $b$（上下文）的条件下，得出 $a$ 的某一个
事实。定义为：

$$f(a,b) = \left \{\begin{array}{cc}
                    1 & \text{$a$ and $b$ satisfy a certain fact} \\
                    0 & \text{otherwise} \end{array} \right }$$


对于特征函数 $f_i$ 它相对与经验概率分布 $\tilde{p}(a,b)$ 的期望为：

$$E_{\tilde{p}}f_i = \sum_{a,b}\tilde{p}(a,b)f_i(a,b)$$

特征函数 $f_i$ 相对于模型 $p(a|b)$ 的期望值为：

$$E_{p}f_i = \sum_{a,b} \tilde{p}(b)p(a|b)f_i(a,b)$$

如果模型能够获取训练数据中的信息，那么就可以假设这两个期望值相等，即：

$$E_{\tilde{p}}f_i = E_{p}f_i$$

上式即称为约束条件。显然，可以定义很多这样的特征函数，它们之间可以是互不相关的。设满足所有约束条件的模型集合为：

$$C \equiv \{ p \in P | E_{\tilde{P}}f_i = E_{p}f_i, i = 1,2,\cdots, k\}$$

定义在条件概率分布
$$p(a|b)$$
上的条件熵为：

$$H(p) =- \sum_{a,b} \tilde{p}(b)p(a|b)\text{log}p(a|b)$$

现在就变成了满足一组约束条件的最优解问题。记 $p^{*} = \text{arg}max_{p \in P} H(p)$ 。求解这个最优解的经典方法是拉格朗日乘子法。
得出：

$$p^{*}(a|b) = \frac{1}{\pi(b)} \text{exp} \left (\sum^k_{i=1}\pi_i f_i(a,b)\right )$$

其中，$\pi(b)$ 是归一化因子

$$\pi(b) = \sum_{a} \text{exp}\left(\sum^k_{i=1} \pi_i f_i(a,b)\right)$$

$\pi_i$ 是参数，可以看成特征函数的权值。如果通过训练集上进行学习，知道了 $\pi_i$ 的值，就得到了概率分布函数，完成了
最大熵模型的构造。



我们对最大熵模型改写为如下形式:

$$p_w(a|b) = \frac{1}{z_w(b)} \text{exp} \left(\sum^k_{i=1} w_i f_i(a, b)\right)$$

其中

$$z_w(b) = \sum_{a} \text{exp}\left(\sum^k_{i=1} w_i f_i(a,b)\right)$$

对数似然函数为：

$$L(w) = \sum_{a,b} \tilde{p}(a,b)\sum^k_{i=1} w_i f_i(a,b) - \sum_{b}\tilde{p}(b)\text{log}z_w(b)$$

并利用改进的迭代尺度法求解最大熵模型。即求对数似然函数的极大值 $\tilde{w}$ .




# Python 实现

{% highlight python %}
import math
import copy

dataset = [['no', 'sunny', 'hot', 'high', 'FALSE'],
           ['no', 'sunny', 'hot', 'high', 'TRUE'],
           ['yes', 'overcast', 'hot', 'high', 'FALSE'],
           ['yes', 'rainy', 'mild', 'high', 'FALSE'],
           ['yes', 'rainy', 'cool', 'normal', 'FALSE'],
           ['no', 'rainy', 'cool', 'normal', 'TRUE'],
           ['yes', 'overcast', 'cool', 'normal', 'TRUE'],
           ['no', 'sunny', 'mild', 'high', 'FALSE'],
           ['yes', 'sunny', 'cool', 'normal', 'FALSE'],
           ['yes', 'rainy', 'mild', 'normal', 'FALSE'],
           ['yes', 'sunny', 'mild', 'normal', 'TRUE'],
           ['yes', 'overcast', 'mild', 'high', 'TRUE'],
           ['yes', 'overcast', 'hot', 'normal', 'FALSE'],
           ['no', 'rainy', 'mild', 'high', 'TRUE']]


class MaxEntropy:
    def __init__(self,maxiter=1000, eps=0.005):
        self._samples = []
        self._Y = set()  # 去重后标签集合
        self._numXY = {}   # key为(x,y), value为出现次数
        self._N = 0    # 样本数
        self._Ep = []  # 样本分布的特征期望值
        self._xyID = {} # key为(x,y), value为index
        self._n = 0  # 特征值(x,y)的个数
        self._C = 0   # 最大特征数
        self._IDxy = {}
        self._w = []
        self._lastw = []
        self._maxiter=maxiter
        self._eps = eps

    def _Zx(self, X):
        """
        计算每个Z(x),规范化因子

        Z_w(x) =\sum_{y} exp(\sum^{n}_{i=1} w_i * f_i(x, y))
        """
        zx = 0
        for y in self._Y:
            ss = 0
            for x in X:
                if (x, y) in self._numXY:
                    ss += self._w[self._xyID[(x, y)]]
            zx += math.exp(ss)
        return zx

    def _model_pyx(self, y, X):
        """
        计算每个P(y|x)
        P_w(y|x) = exp(\sum^n_{i=1} w_i * f_i(x, y)) / z_w(x)
        """
        zx = self._Zx(X)
        ss = 0
        for x in X:
            if (x, y) in self._numXY:
                ss += self._w[self._xyID[(x, y)]]
        pyx = math.exp(ss) / zx
        return pyx

    def _model_ep(self, index):
        """
        计算每个特征函数关于模型的期望 E(f_i) 
        """
        x, y = self._IDxy[index]
        ep = 0
        for sample in self._samples:
            if x not in sample:
                continue
            pyx = self._model_pyx(y, sample)
            ep += pyx / self._N
        return ep

    def _convergence(self):
        """
        判断收敛条件
        """
        for last, now in zip(self._lastw, self._w):
            if abs(last - now) >= self._eps:
                return False
        return True

    def fit(self, X):
        self._samples = copy.deepcopy(dataset)
        for items in self._samples:
            y = items[0]
            X = items[1:]
            self._Y.add(y)
            for x in X:
                if (x, y) not in self._numXY:
                    self._numXY[(x, y)] = 1
                else:
                    self._numXY[(x, y)] += 1

        self._N = len(self._samples)
        self._n = len(self._numXY)
        self._C = max([len(sample) -1 for sample in self._samples])
        self._w = [0] * self._n
        self._lastw = self._w[:]

        self._Ep = [0] * self._n
        for i, xy in enumerate(self._numXY):      # 计算特征值fi关于经验分布的期望
            self._Ep[i] = self._numXY[xy] / self._N
            self._xyID[xy] = i
            self._IDxy[i] = xy

        for _ in range(self._maxiter):
            self._lastw = self._w[:]
            for i in range(self._n):
                ep = self._model_ep(i)
                self._w[i] += math.log(self._Ep[i] /ep ) /self._C
            if self._convergence():
                break

    def predict(self, X):
        Z = self._Zx(X)
        result = {}
        for y in self._Y:
            ss = 0
            for x in X:
                if (x, y) in self._numXY:
                    ss += self._w[self._xyID[(x, y)]]
            pyx = math.exp(ss) / Z
            result[y] = pyx
        return result


if __name__ == '__main__':
    maxent = MaxEntropy()
    x = ['overcast', 'mild', 'high','False']
    maxent.fit(dataset)
    print('predict', maxent.predict(x))

{% endhighlight %}
