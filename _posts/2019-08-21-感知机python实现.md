---
title: "感知机python实现"
date: 2019-08-21
layout: post
categories: 
tags: 
---

# Table of Contents

1.  [概述](#org55e9483)
2.  [模型](#org5e30c5f)
3.  [python实现算法](#org1a044eb)


<a id="org55e9483"></a>

# 概述

深度学习中最基础，我们最早接触的一个基本算法，莫过于感知机模型。它是一种人工神经元，感知机由科学家Frank Rosenblatt发明于
1950至1960年代，他受到了来自Warren McCulloch 和Walter Pitts的更早工作的启发。还有一些其他著名的神经元，例如sigmoid神经元
等。本文只粗略的使用python实现一个感知机。

在《统计学习方法》对感知机是这样介绍的：

> 感知机(perceptron)是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。感知机对应于输入空间
> （特征空间）中将实例划分为正负两类的分离超平面，属于判别类型。感知机学习旨在求出将训练数据进行线性划分的分离超平面。


<a id="org5e30c5f"></a>

# 模型

假设输入空间（特征空间）是 $X \subseteq \mathbb{R}^n$ , 输出空间是 $y=\{+1,-1\}$ ，输入 $x \in X$ 表示实例的
特征向量，对一个于输入空间（特征空间）的点，输出 $y\in Y$ 表示实例的类别，由输入空间到输出空间的如下函数

$$f(x) =\text{sign} (w \cdot x + b)$$

称为感知机。其中，$w$ 和 $b$ 为感知机模型参数， $w\in \mathbb{R}^n$ 叫做全值或权值向量(weight vetor), $b\in \mathbb{R}$
叫做偏置(bias), $w \cdot x$ 表示 $w$ 和 $x$ 的内积，sign是符号函数，即

$$\text{sign}(x) = \begin{cases} +1 & x\geq 0 \\ -1 & x<0 \end{cases}$$

或者我们把二元分类表示成 $\{0,1\}$ ，这样我们改写上面的定义也可以，如在我们引入权重的感念(Rosenblatt提出）。
$w_1,w_2,\ldots$ 等实数来表示各个输入对于输出的重要程度。神经元的输入是0或者1，有加权和 $\sum_j w_j x_j$
是否大于或则会小于某一个阈值(threshold value). 和权重一样，阈值也是一个实数，同时它是神经元的个参数。则：

$$output = \begin{cases} 0 & if \sum_j w_j x_j \leq threshold \\ 1 & if \sum_j w_j x_j > threshold \end{cases}$$

感知机有如下集合解释：线性方程

$$ w \cdot x + b = 0$$

对应于特征空间 $\mathbb{r}^n$ 中的一个超平面s， 其中 $w$ 是超平面的法向量，$b$ 是超平面的截距，这个超平面将特征空间
划分为两个部分。位于两部分的点（特征向量）分别分正、负两类。因此，超平面s称为分离超平面(separating hyperplance).

如果数据线性可分，给定一个数据集，

$$ d =\{(x_1, y_1), (x_2, y_2),\ldots, (x_n, y_n)\}$$

我们的目的就是要找出某个超平面s。

$$ w \cdot x  + b = 0 $$

将数据集d，正确的分离在超平面的两边。为了找到这样的某个超平面，即确定感知机模型参数 $w$, $b$ . 需要定义出经验损失函数
并将其极小化。我们选择误分类点到超平面s的总距离作为损失函数。即输入空间 $\mathbb{r}^n$ 中任意一点到超平s的距离：

$$\frac{1}{||w||_2} | w \cdot x_i + b|$$

对于误分类点来说，

$$-y_i( w \cdot x_i + b) \geq 0$$

因此，误分类点到超平面s的总距离记作，

$$- \frac{1}{||w||_2} \sum_{x_i \in m} y_i (w \cdot x_i + b)$$

从上式，我们自然采用样本点到函数的间隔作为损失函数。

经验风险函数为：

$$l(w, b) = - \sum_{x_i \in m} y_i(w \cdot w_i + b)$$

也即要极小化上式的经验风险函数，这里采用经典的梯度下降法求解 $w$, $b$ . 假设误分类点集合m是固定的，那么损失函数 $l(w, b)$的梯度为

$$\nabla_w l(w, b) = - \sum_{x_i \in m} y_i x_i$$

$$\nabla_b l(w, b) = - \sum_{x_i \in m} y_i$$

随机选取一个误分类点 $(x_i, y_i)$ ，对w, b进行更新：

$$w \leftarrow w + \eta y_i x_i$$

$$b \leftarrow b + \eta y_i$$

其中， $\eta$ 为学习率且 $0< \eta \leq 1$ .
这样通过迭代是损失函数 $L(w,b)$ 不断减小，直到为 0 。


<a id="org1a044eb"></a>

# python实现算法

{% highlight python %}
import numpy as np


class Perceptron:
    def __init__(self, fit_intercept=True):
        """
        A simple perceptron model fit via gradient descent

        Parameters
        ---------
        fit_intercept : bool
             Whether to fit an intercept term in addition to the coefficients
             in  b. If True, the esimates for 'beta' will have 'M + 1'
             dimensions, where the first dimension corresponds to the intercept.
             Default is True.
        """
        self.beta = None
        self.fit_intercept = fit_intercept

    def fit(self, X, y, tol=1e-7, lr=0.01, max_iter=500):
        """
        Fit the regression coefficients via gradient

        ::math:
        The loss function is

               L(X,y) = max(0, -y(w \cdot X +  b))

        If all '(X, y)' was classified correctly, then

              y_i(w \cdot x_i + b) > 0

        Parameter
        -------
        X : NumPy array, shape '(N, M)'
            A dataset consisting of 'N' examples, each of dimension 'M'.
        y : NumPy array, shape '(N, )'
            The binary targets for each of the 'N' examples in 'X'
        tol : float
            When to stop iteration, for the excepted errors rate (loss).
        lr : float
            The gradient learning rate. Default is 0.01
        max_iter : int
            The maximum number of iterations to run the gradient solver.
            Default is 500.
        """
        err_msg = "The dataset and the label must have same dimension"
        assert (X.shape[0] == y.shape[0]), err_msg

        if self.fit_intercept:
            X = np.c_[np.ones(X.shape[0]), X]

        self.target_ = np.unique(y)
        assert self.target_.shape[0] == 2, \
            "label must be two classes!,but there have {}".format(self.target_.shape[0])

        self.beta = np.zeros(X.shape[1])

        self.n_iter = 0
        for _ in range(max_iter):
            y_pred = self._sign(X)
            loss = self._perceptron_loss(y, y_pred)
            if loss < tol:
                return
            self.beta += lr * self._grad(X, y, y_pred)
            self.n_iter += 1

    def score(self, X, y):
        if self.fit_intercept:
            X = np.c_[np.ones(X.shape[0]), X]

        y_pred = self._sign(X)
        acc_ = 1 - self._perceptron_loss(y, y_pred)
        return acc_

    def _sign(self, x):
        """
        The binary targets whether is {-1, +1} or {0, 1}
        """
        return np.where(np.dot(x, self.beta) >= 0, max(self.target_), min(self.target_))

    def _perceptron_loss(self, y, y_pred):
        N = y.shape[0]
        error = y - y_pred
        return np.linalg.norm(error, ord=1) / N

    def _grad(self, X, y, y_pred):
        N = X.shape[0]
        return  np.dot(y - y_pred, X) / N

    def predict(self, X):
        if self.fit_intercept:
            if X.ndim == 1:
                X = np.r_[1, X]
            else:
                X = np.c_[np.ones(X.shape[0]), X]

        return self._sign(X)
{% endhighlight %}

与sklearn中感知机进行对比

{% highlight python %}
from sklearn.linear_model import Perceptron
import perceptron as per

from sklearn.model_selection import train_test_split
from sklearn.datasets.samples_generator import make_blobs

# 创建分类数据
X, y = make_blobs(
    n_samples=100, centers=2, n_features=2, random_state=10
)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

p = per.Perceptron()
p.fit(X_train, y_train)

p_score  = p.score(X_test, y_test)

print("custom Perceptron iter: {}  score : {:.3f}".format(p.n_iter, p_score))

sk_p = Perceptron()
sk_p.fit(X_train, y_train)
sk_p_score = sk_p.score(X_test, y_test)

print("sklearn Perceptron iter: {} score : {:.3f}".format(sk_p.n_iter_, sk_p_score))

{% endhighlight %}

在我随机创建的样本中测试中的结果如下：

custom Perceptron iter: 5  score : 1.000

sklearn Perceptron iter: 7 score : 1.000

迭代的次数与样本情况有关。
