---
title: "利用SVD实现简单的推荐系统"
date: 2019-08-09
layout: post
categories: 
tags: 
---

# Table of Contents

1.  [引言](#org913bfaa)
2.  [SVD定义](#org7321b68)
3.  [利用SVD实现简单的推荐系统](#orgc064485)


<a id="org913bfaa"></a>

# 引言

奇异值分解(Singular Value Decomposition, SVD)是一种被广泛应用的算法，在数据挖掘、机器学习、图像压缩等领域都有着
广泛的应用。SVD是提取信息的强大工具，利用SVD实现，我们能够用小得多的数据集来表示原始的数据集。

SVD主要的应用：

-   隐性语义索引

最早的SVD应用之一就是信息检索，利用SVD的方法为隐性语义索引(Latent Semantic Indexing, LSI)或隐性语义分析(Latent
Seemantic Analysis, LSA)。在LSI中，一个矩阵是由文档词语组成的。当我们在该矩阵上应用SVD时，就会构建多个奇异值。这
些奇异值代表了文档中的概念或主题。这一特点可以用于更高效的文档搜索。

-   推荐系统

SVD的另一个应用就是推荐系统。简单版本的推荐系统能够计算项或者人之间的相似度。更先进的方法则先利用SVD从数据中构建一个
主推空间，然后再在该空间下计算其相似度。

-   矩阵分解

在多数情况下，我们得到数据集中的其中一小段包含整个数据集的大部分信息，其他信息要么是噪声，要么是些无关紧要的信息。利用
SVD，就可以把原矩阵分解成一个易于处理的形式。这种分解出来的形式，是由两个或多个矩阵的乘积，我们可以把这一过程想象成代数
中的因子分解。


<a id="org7321b68"></a>

# SVD定义

先来回顾以下特征值和特征向量的定义：

$$Ax = \lambda x$$

其中 $A$ 是一个 $p\times p$ 的对称矩阵，$x$ 是一个$p \times 1$ 维的向量，那么我们说 $\lambda$ 是矩阵 $A$ 的一个
特征值，而 $x$ 是矩阵 $A$ 的特征值 $\lambda$ 所对应的特征向量。

求出了特征值和特征向量，我们就可以对矩阵 $A$ 进行分解，矩阵 $A$ 的 $p$ 个特征特征值为 $\lambda_1,\lambda_2,\cdots,\lambda_p$ ,

以及所对应的 $p$ 个特征向量 $w_1,w_2,\cdots,w_p$ 。如果这个 $p$ 个特征向量无关，那么矩阵 $A$ 就可以用如下形式表示：

$$A = W \Sigma W^{-1}$$

其中 $W$ 是这个 $p$ 个特征向量所张成的 $p\times p$ 为矩阵，而 $\Sigma$ 为这 $p$ 个特征值为主对角线的 $p \times p$ 维矩阵。

一般我们会把 $W$ 的这 $p$ 特征向量标准化，即满足 $||W_t||_2 = 1$ , 或者 $W^T_t W_t = 1$ , 
此时此时 $W$ 的 $p$ 个特征向量为标准正交基，满足 $W^TW = I$ ，即 $W^T = W^{-1}$ , 也就是说 $W$ 为酉矩阵。

这样我们的特征分解表达式可以写成：

$$A = W \Sigma W^T$$

其中， $A$ 为方阵。

SVD分解不要求分解的矩阵为方阵。假设待分解的矩阵 $A$ 是一个 $p \times q$ 的矩阵，那么我们定义矩阵 $A$ 的SVD为：

$$A = U \Sigma V^T$$

其中， $U$ 为 $p\times q$ 的矩阵， $\Sigma$ 为除了对角线上的元素外其他其他全为0的 $p\times q$ 的矩阵，此时，在对角线
上每个元素都称为奇异值， $V$ 是 $q\times q$ 的矩阵。$U$ 和 $V$ 都是酉矩阵。即满足 $U^T U = I$ , $V^T V= I$ 。

另一个惯例是， $\Sigma$ 的对角元素是从大到小排列的。这些对角线上的奇异值，对应了原始数据集矩阵 $A$ 的奇异值。



$U,\Sigma,V$ 的求解

如果我们将 $A$ 的转置和 $A$ 做矩阵乘法，那么会得到 $q\times q$ 的一个方阵 $A^T A$ 。既然 $A^T A$ 是方阵，那么我们
就可以进行特征分解，得到的特征值和特征向量满足下式：

$$(A^T A)v_i = \lambda_i v_i$$

这样我们就可以得到矩阵 $A^T A$ 的 $q$ 个特征值和对应的 $q$ 个特征向量 $v$ 了。将 $A^T Aa$ 的所有特征向量张成一个
$q\times q$ 的矩阵 $V$ ，就是我们SVD公式里面的 $V$ 矩阵了。一般我们将 $V$ 中的每个特征向量叫做 $A$ 的右奇异向量。

如果我们将 $A$ 和 $A$ 的转置做矩阵乘法，那么会得到 $p\times p$ 的一个方阵 $A A^T$ 。既然 $A A^T$ 是方阵，那么我们
就可以进行特征分解，得到的特征值和特征向量满足下式：

$$(A A^T)u_i = \lambda_i u_i$$

这样我们就可以得到矩阵 $A A^T$ 的 $p$ 个特征值和对应的 $p$ 个特征向量 $u$ 了。将 $A A^T$ 的所有特征向量张成一个
$p\times p$ 的矩阵 $U$ ，就是我们SVD公式里面的 $U$ 矩阵了。一般我们将 $U$ 中的每个特征向量叫做 $A$ 的左奇异向量。

特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：

$$\sigma_i = \sqrt{\lambda_i}$$

旗面提到过，矩阵 $\Sigma$ 只有从大到小排列的对角元素。在科学和工程中，一直存在这样一个普遍事实：在某个奇异值的数目(r个)
个之后，其他的奇异值都置为0.这就意味着数据中仅有r个重要特征，而其余特征都是噪声或冗余特征。
在使用Python过程，我们不必关心 $U,\Sigma,V$ 的求解，在Numpy的线性代数有一个实现了SVD的方法。


<a id="orgc064485"></a>

# 利用SVD实现简单的推荐系统

首先定义相似度的计算方法

{% highlight python %}
def ecludSim(inA, inB):
    return 1.0 / (1.0 + la.norm(inA - inB))


def pearsSim(inA, inB):
    if len(inA) < 3:
        return 1.0
    return 0.5 + 0.5 * corrcoef(inA, inB, rowvar=0)[0][1]


def cosSim(inA, inB):
    num = float(inA.T * inB)
    denom = la.norm(inA) * la.norm(inB)
    return 0.5 + 0.5 * (num / denom)
{% endhighlight %}

其次，利用SVD进行评分估计

{% highlight python %}
def svdEst(dataMat, user, simMeas, item):
    """
    基于SVD的评分估计

    Parameters:
    --------------
    dataMat : 训练数据集
    user : 用户编号
    simMeas : 相似度计算方法
    item : 未评分的物品编号

    Returns:
    ----------------
    评分（0~5之间的值）
    """
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    U, Sigma, VT = la.svd(dataMat)
    Sig4 = mat(eye(4) * Sigma[:4])
    # 利用U矩阵将物品转换到低维空间中，构建转换后的物品（物品的4个主要特征）
    xformedItems = dataMat.T * U[:,:4] * Sig4.I

    for j in range(n):
        userRating = dataMat[user, j]

        if userRating == 0 or j == item:
            continue

        similarity = simMeas(xformedItems[item, :].T, xformedItems[j, :].T)
        print('the %d and %d similarity is: %f' % (item, j, similarity))
        simTotal += similarity
        ratSimTotal += similarity * userRating

    if simTotal == 0:
        return 0
    else:
        return ratSimTotal / simTotal

{% endhighlight %}

三，编写推荐方法

{% highlight python %}
def recommend(dataMat, user, N=3, simMeas=cosSim, estMethod=standEst):
    unratedItems = nonzero(dataMat[user, :].A == 0)[1]
    if len(unratedItems) == 0:
        return ('you rated everything')

    itemScores = []
    for item in unratedItems:
        estimatedScore = estMethod(dataMat, user, simMeas, item)
        itemScores.append((item, estimatedScore))

    return sorted(itemScores, key=lambda jj: jj[1], reverse=True)[:N]
{% endhighlight %}

完整示例代码参见：<https://github.com/makejoyforX/ML_homework/tree/master/svd>
