---
title: "用EM算法实现高斯混合模型"
date: 2019-08-22
layout: post
categories: 
tags: 
---

# Table of Contents

1.  [引言](#orgcd3fb4a)
2.  [高斯混合模型需要考虑的一些细节](#org4550c20)
    1.  [EM收敛补充说明](#org4050aab)
    2.  [高斯似然函数](#org18e1fa3)
    3.  [EM算法中期望最大化计算](#orgbd06dd9)
    4.  [大量浮点相乘造成数据下溢问题](#org96a8a8b)
    5.  [聚类概率估计](#org9b71040)
3.  [Python实现算法](#orgc300ba8)


<a id="orgcd3fb4a"></a>

# 引言

最大期望法（EM）算法在前文有介绍，可以查看 [从极大似然估计到最大期望算法](https://makejoyforx.github.io/blog/2019/08/15/%E4%BB%8E%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%88%B0%E6%9C%80%E5%A4%A7%E6%9C%9F%E6%9C%9B%E7%AE%97%E6%B3%95)


<a id="org4550c20"></a>

# 高斯混合模型需要考虑的一些细节


<a id="org4050aab"></a>

## EM收敛补充说明

需要补充的，EM算法的收敛性证明，这在《统计学习方法》一书中有较为详尽的说明，需要加以认真考察的是下列式子的计算：

$$Q(\theta, \theta^{(i)}) = \sum_{z} \log P(Y,Z \mid \theta) P(Z \mid Y, \theta^{(i)})$$

也即我们令假设写成：

$$H(\theta, \theta^{(i)}) = \sum_{z} \log P(Z \mid Y, \theta) P(Z \mid Y, \theta^{(i)})$$

在如下条件：

$$P(Y \mid \theta^{(i+1)}) \geq P(Y \mid \theta^{(i)})$$

(似然函数单调递增）

计算我们上下界之差小于某一阈值，我们认为达到我们需要求得似然函数的目的，即如下所示：

$$\log P(Y\mid \theta) = Q(\theta, \theta^{(i)}) - H(\theta, \theta^{(i)})$$

在此式中取 $$\theta$$ 为 $\theta^{(i)}$ 和 $\theta^{(i+1)}$

$$Q(\theta^{(i+1)} , \theta^{(i)}) - Q(\theta^{(i)}, \theta{(i)}) \geq 0$$

如果上式之差小于某一阈值，就停止迭代，认为得到了的然函数 $P(Y \mid \theta)$ 的最大值。

具体在python实现如下：

{% highlight python %}
def _likelihood_lower_bound(self):
    """
    Compute the likelihood lower bound under current parametes
    """
    N = self.N
    C = self.C

    eps = np.finfo(float).eps
    expec1, expec2 = 0.0, 0.0
    for i in range(N):
        x_i = self.X[i]

        for c in range(C):
            pi_k = self.pi[c]
            z_nk = self.Q[i, c]
            mu_k = self.mu[c, :]
            sigma_k = self.sigma[c, :, :]

            log_pi_k = np.log(pi_k + eps)
            log_p_x_i = _log_gaussian_pdf(x_i, mu_k, sigma_k)
            prob = z_nk * (log_pi_k + log_p_x_i)

            expec1 += prob
            expec2 += z_nk * np.log(z_nk + eps)

    loss = expec1 - expec2
    return loss
{% endhighlight %}


<a id="org18e1fa3"></a>

## 高斯似然函数

另外在计算高斯密度函数的对数似然比较复杂，引用《APPLIED MULTIVARIATE STATISTICAL ANALYSIS》一书对高斯密度函数的表述。

高斯密度函数 

$$f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{[(x-\mu)/\sigma]^2 / 2} , -\infty <x < \infty$$

利用向量，矩阵的形式来表示为：

$$f(x) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} e^{ (x-\mu)'\Sigma^{-1}(x-\mu)/2}$$

我们的高斯似然函数可以写成如下

{% highlight python %}
def Gaussian_(X, mu, Sigma):
    """
    Redefine Gaussian density function
    f(X) = frac{1}{(2pi)^{d/2} * |Sigma|^{1/2}} * exp (- (X-mu)'Sigma^{-1}(x-mu)/2 )
    """
    d = np.shape(Sigma)[0]

    try:
        det = np.linalg.det(Sigma)
        s_inv = np.linalg.inv(Sigma)
    except np.linalg.LinAlgError:
        print("singular matrix: collapsed")

    x_diff = (X - mu).reshape((1, d))

    prob = 1.0 /(np.power(np.power(2 * np.pi, d) * np.abs(det), 0.5)) * \
        np.exp(-0.5 * x_diff.dot(s_inv).dot(x_diff.T) )[0][0]

    return prob
{% endhighlight %}

不过上式涉及到大量的相乘计算，可能无法得到很好的效果。于是想到可以对高斯密度函数求导，计算对数似然函数。

$$ L(\mu,\sigma^2) = \prod^N_{i=1} \frac{1}{\sqrt{2\pi \sigma^2}} e^{- \frac{(x_i - \mu)^2}{2\sigma^2}}
= (2\pi \sigma^2)^{- \frac{n}{2}} e^{-\frac{1}{2\sigma^2} \sum^n_{i=1} (x_i - \mu)^2}$$

对数似然似然为：

$$ \log L(\mu,\sigma^2) = -\frac{n}{2}\log (2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum^n_{i=1}(x_i - \mu)^2$$

进一步，我们改写成

$$ \log p(X \mid \mu, \Sigma) = - \frac{n}{2} log(2\pi) - \frac{n}{2} \log |\Sigma| - \frac{1}{2} \sum^n_{i=1}(x_i -\mu)'\Sigma^{-1}(x_i - \mu)$$

于是，我们的高斯对数似然函数写为如下：

{% highlight python %}
def _log_gaussian_pdf(x_i, mu, sigma):
    """
    Compute log(x_i | mu, sigma)
    """
    n = len(mu)
    a = n * np.log(2 * np.pi)
    _, b = np.linalg.slogdet(sigma)

    y = np.linalg.solve(sigma, x_i - mu)
    c = np.dot(x_i - mu, y)
    return -0.5 * (a + b + c)
{% endhighlight %}


<a id="orgbd06dd9"></a>

## EM算法中期望最大化计算

聚类当中每一类的权重计算，显然与每一类的密度有关，计算权重比较简单，简单除以每一类的密度即可。
高斯密度函数中均值和协方差矩阵的最大期望计算稍微比较复杂，不过一样的，也是与聚类中，每一类相关，即我们需要与每一个类（或称为k个高斯组件）
的密度关联起来。

计算均值

{% highlight python %}
nums_mu = [np.dot(self.Q[:, c], X) for c in range(C)]
for ix, (num, den) in enumerate(zip(nums_mu, denoms)):
    self.mu[ix, :] = num / den if den > 0 else np.zeros_like(num) # denoms 为每一聚类的密度
{% endhighlight %}

计算协方差矩阵

{% highlight python %}
for c in range(C):
    mu_c = self.mu[c, :]
    n_c = denoms[c]

    outer = np.zeros((self.d, self.d))
    for i in range(N):
        wic = self.Q[i, c]
        xi = self.X[i, :]
        outer += wic * np.outer(xi - mu_c, xi - mu_c)

    outer = outer / n_c if n_c > 0 else outer
    self.sigma[c, :, :] = outer
{% endhighlight %}


<a id="org96a8a8b"></a>

## 大量浮点相乘造成数据下溢问题

在进行密度计算等场景下，需要多个概率连乘，这有可能造成浮点下溢问题。在《统计自然语言处理》有介绍，例如讲解隐马尔可夫模型中，Viterbi算法中
采用乘法运算求最大值问题，采用对概率相乘的算法取对数运算，使乘法运算变成加法运算。虽然本文对高斯混合模型的计算采取了这一方法，但在涉及密度
计算( $Q(\theta)$ 的计算)仍然需要考虑数据下溢的问题。
因此我们采用一种称为 Log Sum Exponential 的技巧。

$$ \log \sum^n_{i=1} exp (x_i) = \log exp(b) \sum^n_{i=1} exp(x_i - b) = b + \log \sum^n_{i=1} exp(x_i - b)$$


<a id="org9b71040"></a>

## 聚类概率估计

在我们计算出隐变量均值和协方差矩阵的似然估计后，用估计需要预测的数据的类别判断就十分容易了，利用隐变量来反向传播计算需要计算的数据，只需返回
计算出的概率最大值的索引即可。

{% highlight python %}
def predict(self, X):
    prob = self.predict_proba(X)
    return np.argmax(prob, axis=1)
{% endhighlight %}


<a id="orgc300ba8"></a>

# Python实现算法

完成的高斯混合模型算法Python实现可以参见

<https://github.com/makejoyforX/ML_homework/tree/master/mixture>
