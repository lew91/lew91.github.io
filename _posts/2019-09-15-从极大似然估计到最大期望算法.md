---
title: "从极大似然估计到最大期望算法"
date: 2019-08-15
layout: post
mathjax: true
categories: 
tags: 
---

# Table of Contents

1.  [引言](#org3040e32)
2.  [极大似然估计（MLE）](#org110cd52)
    1.  [极大似然估计的概率](#orge618710)
    2.  [似然函数](#org03ca1b8)
    3.  [最大后验概率（MAP）](#org2cc6e2d)
    4.  [极大似然估计的步骤](#org1fbc4a2)
3.  [最大期望算法（EM）](#org6460d18)


<a id="org1e36a42"></a>

# 引言

概率和统计是数据挖掘、机器学习领域中两大利器。正确且熟练掌握它们十分重要，本文暂且记录一下，对这些基本知识的学习小结。
极大似然估计（MLE）及其最大期望算法（EM）在机器学习当中经常被用到，例如线性回归，逻辑回归、贝叶斯线性回归、岭回归及其
一些混合模型参数估计、隐马尔可夫模型参数估计等等。
而在概率和统计这两者之间，有时可能会搞混它们彼此的概率。基本来讲，概率和统计研究的问题相反。概率研究的问题是，已知模型
和参数，怎么去预测模型结果的特性（例如均值，方差等）。而统计研究的问题刚好相反，统计是得到了数据后，去估计模型和参数。
极大似然估计（MLE）是已知数据的情况下，估计模型参数的方法，而最大期望算法则是在包含隐变量(latent variable)的情况下
的极大似然估计或极大后验概率估计。


<a id="orgf3c9503"></a>

# 极大似然估计（MLE）


<a id="org13c2597"></a>

## 极大似然估计的概率

《统计自然语言处理》对最大似然估计的定义如下

>如果 ${s_1,s_2,...,s_n\}$ 是一个试验的样本空间，在相同的情况下重复试验N次，观测到样本 $s_k(1\leq k \leq n)$
>的次数为 $n_N(s_k)$ , 那么，$s_k$ 在这N次试验中的相对频率为
>
>$$q_{N} (s_k) = \frac{n_{N}(s_k)}{N}$$
>
>由于 $\sum^n_{k=1} n_N(s_k) = N$ , 因此， $\sum^n_{k=1} q_N(s_k) = 1$ 。
>
>当N越来越大时，相对频率 $q_N(s_k)$ 就越来越接近 $s_k$ 的概率 $P(s_k)$ . 事实上，
>
>$$\lim_{N \rightarrow \infty } q_N (s_k) = P(s_k)$$
>
>因此，通常用相对频率作为概率的估计值。这种估计概率值的方法称为最大似然估计(likelihood estimation).

也就是说，最大似然估计是利用已知的样本结果，反推出最有可能（最大概率）导致这样的结果的参数值。换句话说，就是“模型已知，参数
未知”。我们进行若干次试验，观察其结果，利用试验结果得到某个参数值是样本出现的概率最大，则称为最大似然估计。


<a id="orgacb0861"></a>

## 似然函数

再来回归一下贝叶斯公式：
$$P(B|A) = \frac{P(B \cap A)}{P(A)} =\frac{ P(A|B)P(B)}{P(A)}$$

全概率公式 《统计自然语言处理》引用

>假设A为样本空间 $\Omega$ 的事件，$B_1,B_2,\cdots,B_n$ 为 $\Omega$ 的一个划分，如果 $A \subseteq \cup^n_{i=1} B_i$ ,
>P(A)>0, 并且 $i\neq j$ , $b_i \cap B_j = \oslash$ ，$P(B_i) > 0 (i=1,2,...,n)$, 则
>
>$$P(B_j | A) = \frac{P(A |  B_j)P(B_j)}{P(A)} = \frac{P(A|B_j)P(B_j)}{\sum^n_{i=1}P(A|B_i)P(B_i)}$$

上式被称为全概率公式，从贝叶斯全概率公式可以看出，我们做判断的时候，要考虑所有因素。

我们换一种形式的写法

$$P(\theta \mid x) = \frac{P(x\mid \theta)P(\theta)}{P(x)}$$

其中，$P(\theta)$ 为先验概率，表示每个类别分布的概率， $P(x\mid \theta)$ 为条件概率，表示某事发生的概率，而 $P(\theta\mid x)$
为后验概率，表示某事发生，并且它属于某一类的概率。

似然函数在如下给出
由于样本集中的样本都是独立同分布，可以只考虑一类样本集D，来估计参数向量 $\theta$ . 记已知的样本集为：

$$ D = \{x_1,x_2,\cdots,x_N \}$$

似然函数(likelihood function):条件概率密度函数 $p(D|\theta)$ 称为相对于 $\{x_1,x_2,\cdots,x_N\}$ 的 $\theta$ 的似然函数。

$$ l(\theta) = p(D|\theta) = p(x_1,x_2,\cdots,x_N | \theta) =\prod^N_{i=1} p(x_i | \theta)$$

如果 $\hat{\theta}$ 是参数空间中使似然函数 $l(\theta)$ 最大的 $\theta$ 值，则 $\hat{\theta}$ 应该是“最可能”的参数值，那么 
$\hat{\theta}$ 就是 $\theta$ 的极大似然估计量。它是样本集的函数，记作：

$$\hat{\theta} = d(x_1,x_2,\cdots,x_N) = d(D)$$

$$\hat{\theta}(x_1,x_2,\cdots,x_N)$$ 

称作极大似然函数估计值


求解极大似然函数（极大似然估计就是求解概率最大的 $\theta$ 值）
ML估计：求使得出现该组样本的概率最大的 $\theta$ 值。

$$\hat{\theta} = \text{arg} max_{\theta} l(\theta) = \text{arg} max_{\theta} \prod^N_{i=1} p(x_i | \theta)$$

实际中为了便于分析，定义了对数似然函数：

$$ H(\theta) = \log l(\theta)$$

$$\hat{\theta} = \arg max_{\theta} H(\theta) = \arg max_{\theta}\log l(\theta) 
= \arg max_{\theta} \sum^N_{i=1} \log p(x_i | \theta)$$

特别的，如果右端再除以N，就变成统计学上的期望

$$ \theta = \arg max_{\theta} \left(\sum^N_{i=1} \log p(x_i\mid theta)\right)/N = \arg max_{\theta} E[\log p(x_i \mid \theta)]$$

1.未知参数只有一个（ $\theta$ 为标量）
在似然函数满足连续、可微的正则条件下，极大似然估计量是下面微分方程的解：
$\frac{dl(\theta)}{d\theta} = 0$ 或等价于 $\frac{dH(\theta)}{d\theta} = \frac{d \text{ln}l(\theta)}{d\theta} = 0$

2.未知参数有多个 ( $\theta$ 为向量)
则 $\theta$ 可表示为具有S个分量的未知向量：

$$ \theta = \left [ \theta_1, \theta_2, \cdots, \theta_S \right ]^T$$

记梯度算子：

$$ \nabla_{\theta} = \left[ \frac{\partial}{\partial \theta_1}, \frac{\partial}{\partial \theta_2},\cdots,\frac{\partial}{\partial \theta_S} \right]^T$$

若似然函数满足连续可导的条件，则最大似然估计量就是如下方程的解。

$$ \nabla_{\theta} H(\theta) = \nabla_{\theta} \log l(\theta) = \sum^N_{i=1} \nabla_{\theta} \log p(x_i | \theta) = 0$$

方程的解只是一个估计值，只有在样本数趋于无限多的时候，它才会接近真实值。


<a id="org4aa8c6d"></a>

## 最大后验概率（MAP）

最大似然估计是求参数 $\theta$， 使得似然函数 $P(x\mid \Theta)$ 最大。最大后验概率（MAP）则是求 $\theta$ 使 $P(x \mid \theta)P(\theta)$
最大，求得的 $\theta$ 不单单使得似然函数最大，$\theta$ 自身出现的先验概率也最大。

MAP要使的 $P(\theta \mid x)$ 最大化，即：

$$ \arg max_{\theta} \frac{P(x\mid \theta)P(\theta)}{P(x)}  = \arg max_{\theta}P(x \mid \theta)P(\theta)$$


<a id="org94eb341"></a>

## 极大似然估计的步骤

-   写出似然函数
-   对似然函数取对数
-   求导数，令导数为0，得到似然方程
-   解似然方程，得到参数


<a id="org4837e8c"></a>

## 极大似然估计在逻辑回归中的使用

使用l2正则化或l1正则化惩罚的情况下,求误差

{% highlight python %}
def _NLL(self, X, y, y_pred):
    """
    Penalized negative log likelihood

    此时  y_pred 由 sigmoid 函数得到
    y_pred = sigmoid(np.dot(X, self.beta))
    """
    N, M = X.shape
    order = 2 if self.penalty == "l2" else 1
    nll = np.log(y_pred[y == 1]).sum() - np.log(1 - y_pred[y == 0]).sum()
    penalty = 0.5 * self.gamma * np.linalg.norm(self.beta, ord=order) ** 2
    return (penalty + nll) / N 
{% endhighlight %}

使用梯度下降法求解权重

{% highlight python %}
def _NLL_grad(self, X, y, y_pred):
    """
    Grandient of the penalized negative log likelihood wrt beta

    X 有作变换
    X = np.c_[np.ones(X.shape[0]), X]
    """
    N, M = X.shape
    p = self.penalty
    beta = self.beta
    gamma = self.gamma
    l1norm = lambda x: np.linalg.norm(x, 1)
    d_penalty = gamma * beta if p == "l2" else gamma * l1norm(beta) * np.sign(beta)
    return -(np.dot(y - y_pred, X) + d_penalty) / N 
{% endhighlight %}

完整程序：


<https://github.com/makejoyforX/ML_homework/blob/master/logistic_regression/LogisticRegression.py>


<a id="org747256f"></a>

# 最大期望算法（EM）


<a id="org09849de"></a>

## EM简要概念

EM算法是一种迭代算法，1997年由Dempster等人总结提出，用于含有隐变量(latent variable)的概率参数模型的最大似然估计或极
大后验概率估计。

一般地，用X表示观察随机变量的数据，Z表示隐随机变量的数据。X和Z连在一起称为完全数据(complete-data)，观察数据X又称不完全
数据(incomplete-data)。假设给定观测数据X，其概率分布是 $P(X\mid \theta)$ ，其中 $\theta$ 是需要估计的模型参数，那么
不完全数据X的似然函数是 $P(X\mid \theta)$ ，对数似然函数 $L(\theta)= \log P(X \mid \theta)$ ；假设X和Z的联合概率
分布是 $(X,Z \mid \theta)$， 那么完全数据的对数似然函数是 $\log P(X,Z \mid \theta)$ 。

EM算法通过迭代求 $L(\theta)= \log P(X \mid \theta)$ 的极大似然估计。每次迭代包含两步：E步，求期望；M步，求极大化。


>输入：观测变量数据X，隐变量数据Z，联合分布 $P(X,Z \mid \theta)$ ，条件分布 $P(Z \mid X,\theta)$ :
>
>输出：模型参数 $\theta$
>
>1.选择参数的初始值 $\theta^{(0)}$ , 开始迭代；
>
>2.E步：记 $\theta^{(i)}$ 为第i次迭代的参数 $\theta$ 的估计值，在第 i+1次迭代的E步，计算 
>
>$$Q(\theta, \theta^{(i)}) = E_{Z}[\log P(X,Z \mid \theta \mid X, \theta^{(i)})]$$
>
>$$ = \sum_{Z} \log P(X,Z \mid \theta) P(Z \mid X, \theta^{(i)})$$
>
>这里，$P(Z\mid X,\theta^{(i)})$ 是在给定观测数据X和当前参数估计 $\theta^{(i)}$ 下隐变量数据Z的条件概率分布；
>
>3.M步：求使 $Q(\theta,\theta^{(i)})$ 极大化的 $\theta$ ,确定第 i+1 次迭代的参数的估计 $\theta^{(i+1)}$ 
>
>$$\theta^{(i+1)} = \arg max_{\theta} Q(\theta, \theta^{(i)})$$
>
>4. 重复第2步和第3步，直到收敛。
>
>
>迭代停止条件
>
>$$||\theta^{(i+1)} - \theta^{(i)}|| <\varepsilon_1$$
>
>或 
>
>$$||Q(\theta^{(i+1)},\theta^{(i)}) - Q(\theta^{(i)},\theta^{(i)})|| < \varepsilon_2$$
>


<a id="org42745ed"></a>

## EM推导

由于模型中含有一个隐变量，我们的目标是极大化观测数据（不完全数据）X关于参数 $\theta$ 的对数似然函数，即 

$$L(\theta) = \log P(X\mid \theta) = \log \sum_{Z} P(X,Z \mid \theta)$$

$$=\log \left(\sum_{Z} P(X\mid Z, \theta)P(Z\mid \theta)\right)$$

上式极大化的主要困难是包含未观测数据并有包含和（或积分）的对数。

对于每个实例i，用 $Q_i$ 表示样本实例隐变量z的某种分布，且 $Q_i$ 满足条件 $(\sum_{Z} Q_i(z)=1, Q_i(Z) \geq 0)$
若 $Q_i$ 是连续性的，则 $Q_i$ 表示概率密度函数，需要将求和符号换成积分符号。

现在，我们上面公式改写：

$$\sum_{i} \log P(X\mid \theta) = \sum_i \log \sum_{z^{(i)}} P(x^{(i)} , z^{(i)} \mid \theta)$$

把式中的log函数看成一个整体，有与log(x)的二阶导数为 $- \frac{1}{x^2}$ ,小于0，为凹函数

利用琴生不等式 ( $f(E(X) \geq E[f(x))]$ )

$$ \sum_i \log \sum_{z^{(i)}} Q_i (z^{(i)}) \frac{P(x^{(i)},z^{(i)} \mid \theta)}{Q_i (z^{(i)})}$$

$$\geq \sum_i \sum_{z^{(i)}} Q_i (z^{(i)})\log \frac{P(x^{(i)},z^{(i)} \mid \theta)}{Q_i(z^{(i)})}$$

因此，化解为求随机变量的期望。

参考 “Applied Multivariate statistical analXsis”, sixth edition。求均值，方差的方程式。

令 y=g(x),那么

$$ E[x]= E[g(x)] = \begin{cases} \sum^{\infty}_{i=1} p_i g(x_i) & x为离散随机变量，概率分布为p \\
                                         \int^{\infty}_{-\infty}g(x)f(x)dx & x为连续分布，概率密度函数为 f(x)\end{cases}$$

类似的，把对数似然函数的 $Q_i(z^{(i)})$ 看作相应的概率 $p_i$ , 把 $\frac{P(x^{(i)}, z^{(i)} \mid \theta)}{Q_i(z^{(i)})}$ 看作是
$z^{(i)}$ 的函数 g(z).

根据 $E(X) = \sum X * P(X)$ 得到

$$\sum_{z^{(i)}} Q_i(z^{(i)}) \left[\frac{P(x^{(i)}, z^{(i)}\mid \theta)} {Q_i(z^{(i)})}\right]$$


根据统计学lazy原则，上式其实就是 $\frac{P(x^{(i)}, z^{(i)} \mid \theta)}{Q_i(z^{(i)})}$ 的期望。
即


>Lazy Statistician 规则
>设Y是随机变量X的函数，
>
>$$ Y=g(x)$$
>
>g是连续函数
>
>X是离散型随机变量，它的分布律为
>
>$ P(X=x_k) = p_k$ ， 其中k=1,2..., 若 
>
>$$ \sum^{\infty}_{k=1} g(x) p_k $$
>
>的值绝对收敛，则有
>
>$$E(Y) = E(g(X)) = \sum^{\infty}_{k=1} g(x_k)p_k$$


对应于上述问题，Y是 $\left[P(x^{(i)},z^{(i)} \mid \theta) / Q_i(z^{(i)})\right]$，
$X$ 是 $z^{(i)}$ , $Q_i(z^{(i)})$ 是 $p_k$ , $g$ 是 $z^{(i)}$ 到 y的映射。

再根据凹函数时的Jensen不等式：

$$f \left(E_{z^{(i)}\sim Q_i} \left[\frac{P(x^{(i)}, z^{(i)}\mid \theta)}{Q_i(z^{(i)})}\right]\right) 
\geq E_{z^{(i)}\sim Q_i} \left[f \left(\frac{P(x^{(i)}, z^{(i)}\mid \theta)}{Q_i(z^{(i)})}\right)\right]$$


此时我们可以表述 $L(\theta) \geq J(z,Q)$ ,这样我们就可以固定 $\theta$ 值，是 下界 $J(z,Q)$ 不断逼近 $L(\theta)$ .
直到收敛到似然函数 $L(\theta)$ 的最大值处 $\hat{\theta}$ 。


<a id="orgd8e2db7"></a>

## EM在高斯混合模型及隐马尔可夫算法中应用

为防止大量浮点算法导致下溢问题，重新定义函数 logsumexp(), 当然也可以直接使用SciPy提供的logsumexp()函数，直接导入即可

{% highlight python %}
from scipy.special import logsumexp
{% endhighlight %}

logsumexp函数

{% highlight python %}
def logsumexp(log_probs, axis=None):
    _max = np.max(log_probs)
    ds = log_probs - _max
    exp_sum = np.exp(ds).sum(axis=axis)
    return _max + np.log(exp_sum)
{% endhighlight %}

高斯混合模型中EM算法的E步和M步

{% highlight python %}
def _E_step(self):
        for n in range(self.N):
            x_n = self.X[n, :]

            denom_vals = []
            for c in range(self.C):
                a_c = self.alpha[c]
                mu_c = self.mu[c, :]
                sigma_c = self.sigma[c, :, :]

                posterior= np.log(a_c) + np.log( Gaussian_(x_n, mu_c, sigma_c))
                denom_vals.append(posterior)

            log_denom = logsumexp(denom_vals)
            gamma = np.exp([num - log_denom for num in denom_vals])
            assert_allclose(np.sum(gammas), 1, err_msg="{}".format(np.sum(gammas)))

            self.gammas[n, :] = gamma

    def _M_step(self):
        C, N, X = self.C, self.N, self.X
        denoms = np.sum(self.gammas, axis=0)

        # update cluster priors
        self.alpha = denoms / N

        # update cluster means
        nums_mu = [np.dot(self.gammas[:, c], X) for c in range(C)]
        for ix, (num, den) in enumerate(zip(nums_mu, denoms)):
            self.mu[ix, :] = num / den if den > 0 else np.zeros_like(num)

        # update cluster covariances
        for c in range(C):
            mu_c = self.mu[c, :]
            n_c = denoms[c]

            outer = np.zeros((self.d, self.d))
            for i in range(N):
                wic = self.gammas[i, c]
                xi = self.X[i, :]
                outer += wic * np.outer(xi - mu_c, xi - mu_c)

            outer = outer / n_c if n_c > 0 else outer
            self.sigma[c, :, :] = outer

        assert_allclose(np.sum(self.alpha), 1, err_msg="{}".format(np.sum(self.alpha)))
{% endhighlight %}

完整代码参见：

<https://github.com/makejoyforX/ML_homework/blob/master/EM/gmm_lihang.py>

隐马尔可夫模型算法比较复杂，设计到Baum-Welch算法（或者称为Forward-Backward 前向后向算法），Viterbi算法，和EM算法来估计参数，这里不再给出，
感兴趣的读者可以参阅sklearn资料。
